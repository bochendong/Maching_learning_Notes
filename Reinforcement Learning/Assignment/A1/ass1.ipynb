{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {
    "id": "tKxQjeye8UML"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "\n",
    "class BanditPolicy():\n",
    "    \"\"\"\n",
    "    This is the base/parent policy class, not meant to be used directly.\n",
    "\n",
    "    We place this at the top of the file so that anyone reading this code in \n",
    "    future - including future you - will know that to use any policy class, they\n",
    "    mainly just need to read this one. Read one thing, and learn most of how\n",
    "    all the others work. So convenient!!\n",
    "    \"\"\"\n",
    "\n",
    "    # We don't need an init, since this is about the shared methods of choice.\n",
    "\n",
    "    def argmax_with_random_tiebreaker(self, action_value_estimates):\n",
    "        \"\"\"\n",
    "        Chooses the maximum of the provided action-value estimates,\n",
    "        with ties broken randomly.\n",
    "\n",
    "        Args:\n",
    "            action_value_estimates: A numpy array containing action-value\n",
    "            estimates.\n",
    "        Returns:\n",
    "            The index of the max element.\n",
    "        \"\"\"\n",
    "        return np.random.choice(\n",
    "            np.where( action_value_estimates == action_value_estimates.max())[0]\n",
    "        )\n",
    "    \n",
    "    def choose_action(self):\n",
    "        \"\"\"\n",
    "        This method is just here to be overridden, so it is 'empty'.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class BanditEpsilonGreedyPolicy(BanditPolicy):\n",
    "    \"\"\"\n",
    "    The epsilon-greedy action selection policy. An agent following the \n",
    "    epsilon-greedy policy will choose a random action with probability epsilon,\n",
    "    and will greedily choose the best action (argmax) with probability \n",
    "    1 - epsilon. If multiple actions are tied for the best choice, ties are\n",
    "    broken randomly.\n",
    "    \n",
    "    Attributes:\n",
    "        epsilon: A value [0,1] that determines the probability that an agent will\n",
    "        randomly choose an action at each timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon = 0.1):\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Epsilon-Greedy Policy: {e}\".format(e = self.epsilon)\n",
    "    \n",
    "    def choose_action(self, agent_data):\n",
    "        \"\"\"\n",
    "        This is where we override the method that's in the base class.\n",
    "\n",
    "        Args:\n",
    "            action_value_estimates: A numpy array containing the action-value\n",
    "            estimates for a given bandit problem environment.\n",
    "        Returns:\n",
    "            action: The index of the chosen action.\n",
    "        \"\"\"\n",
    "        action_value_estimates = agent_data['action_value_estimates']\n",
    "\n",
    "        roll = random.uniform(0,1)\n",
    "        \n",
    "        if roll <= self.epsilon:\n",
    "            action = random.choice( list( range(0,len(action_value_estimates))))\n",
    "        else:\n",
    "            action = self.argmax_with_random_tiebreaker(action_value_estimates)\n",
    "        return action\n",
    "    \n",
    "\n",
    "class BanditUCBPolicy(BanditPolicy):\n",
    "    \"\"\"\n",
    "    EXPLANATION OF UCB GOES HERE\n",
    "    \n",
    "    ALSO INCLUDE INPUT / OUTPUT DOCUMENTATION\n",
    "    \"\"\"\n",
    "    def __init__(self, c):\n",
    "        self.c = c\n",
    "        self.t = 1\n",
    "\n",
    "    def __str__(self):\n",
    "      return \"UCB Policy, C = {c}\".format(c = self.c)\n",
    "\n",
    "    def choose_action(self, agent_data):\n",
    "\n",
    "\n",
    "        \n",
    "        # CODE\n",
    "        action_value_estimates = agent_data[\"action_value_estimates\"]\n",
    "        action_counts = agent_data[\"action_counts\"]\n",
    "\n",
    "        ucb_values = []\n",
    "        for i, n in enumerate(action_counts):\n",
    "            if n == 0:\n",
    "                ucb_values.append(np.inf)\n",
    "            else:\n",
    "                ucb_value = action_value_estimates[i] + self.c * np.sqrt(np.log(self.t) / n)\n",
    "                ucb_values.append(ucb_value)\n",
    "\n",
    "        ucb_values = np.array(ucb_values)\n",
    "        # print(ucb_values)\n",
    "        action = self.argmax_with_random_tiebreaker(ucb_values)\n",
    "        self.t += 1\n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "class BanditSoftmaxPolicy(BanditPolicy):\n",
    "    \"\"\"\n",
    "    As in equation 2.9 in the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Softmax Policy\"\n",
    "\n",
    "    def softmax(self, x):\n",
    "        probabilities = np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
    "        return probabilities\n",
    "\n",
    "    def choose_action(self,agent_data):\n",
    "        action_value_estimates = agent_data[\"action_value_estimates\"]\n",
    "        probabilities = self.softmax(action_value_estimates)\n",
    "        action_choices = range(0,len(action_value_estimates))\n",
    "        action = random.choices(action_choices, probabilities)[0]\n",
    "        return action\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "IlIpLETL5Jzy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "class BanditEnvironment():\n",
    "    \"\"\"\n",
    "    The K-Armed Bandit. I called this \"BanditEnvironment\" because the\n",
    "    environment is the problem the learner agent is trying to solve; it is\n",
    "    the thing they act upon that gives reward feedback.\n",
    "\n",
    "    Attributes:\n",
    "        k: The number of actions (\"levers\", \"arms\") in the bandit problem.\n",
    "        mu: The mean of the normal distribution from which we draw q*, the \n",
    "            true action value for each arm. Note that we don't have to draw\n",
    "            q* from a normal distribution, and might work that into this class\n",
    "            or a new class if we wanted to.\n",
    "        sigma: The standard deviation of the above distribution.\n",
    "        true_action_values: q*, the true action values.\n",
    "        optimal action: The action index that has the highest q*.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, mu, sigma):\n",
    "        self.k = k\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.true_action_values = np.random.normal(mu, sigma, k)\n",
    "        self.optimal_action = np.random.choice(\n",
    "            np.where(self.true_action_values == self.true_action_values.max())[0]\n",
    "        )\n",
    "    \n",
    "    def reset_true_action_values(self):\n",
    "        \"\"\"\n",
    "        Resets the values of all actions to 0. This is for resets between\n",
    "        runs, or if you want to see how your learner responds to a \"system\n",
    "        shock\" where the dynamics change suddenly. If you want to provide a \n",
    "        more serious shock than the BanditEnvironment object's current mu\n",
    "        and sigma, you can change these manually, e.g.,\n",
    "        my_bandit_env.mu = 5, etc.\n",
    "        \"\"\"\n",
    "        self.true_action_values = np.random.normal(self.mu, self.sigma, self.k)\n",
    "        self.optimal_action = np.random.choice(\n",
    "            np.where(self.true_action_values == self.true_action_values.max())[0]\n",
    "        )\n",
    "\n",
    "    def random_walk_action_values(self, mu = 0, sigma = 0.01):\n",
    "        \"\"\"\n",
    "        A method that adds a random value to each element of the true action\n",
    "        values. \n",
    "\n",
    "        This is for part of Assignment 1! (Exercise 2.5 in the text)\n",
    "\n",
    "        Args:\n",
    "            mu: The mean of the normal distribution from which the random\n",
    "                walk value is drawn.\n",
    "            sigma: The standard deviation of the above distribution.\n",
    "        \"\"\"\n",
    "        draws = np.random.normal(mu,sigma, len(self.true_action_values))\n",
    "        self.true_action_values += draws\n",
    "        self.optimal_action = np.random.choice(\n",
    "            np.where(self.true_action_values == self.true_action_values.max())[0]\n",
    "        )\n",
    "\n",
    "    def take_action_emit_reward(self, action):\n",
    "        \"\"\"\n",
    "        Accepts an action index, draws reward from that q* distribution, and\n",
    "        outputs a reward for that action + whether or not that action was\n",
    "        optimal.\n",
    "\n",
    "        Args:\n",
    "            action: The index of the action to take.\n",
    "        Returns:\n",
    "            reward: A scalar (float) reward value.\n",
    "        \"\"\"\n",
    "        return np.random.normal( self.true_action_values[action]), \\\n",
    "            action == self.optimal_action\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "QmMUoaUM5RKw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class BanditAgent():\n",
    "    \"\"\"\n",
    "    This is a base class that will not be explicitly used in code.\n",
    "\n",
    "    So far we have learned that there are various ways of estimating q*. We\n",
    "    focused on the sample average method so far in our coding, but of course\n",
    "    there are other ways like the gradient bandit algorithm.\n",
    "\n",
    "    Because agents all have things in common - choosing actions according to\n",
    "    a policy, action-value estimates, updating, etc. - we can put the common\n",
    "    factors in this base class or \"parent\" class, and have the other agent\n",
    "    classes just inherit. This keeps our code cleaner and more flexible.\n",
    "\n",
    "    Attributes:\n",
    "        policy: An instance of the BanditPolicy class set. Used for choosing\n",
    "                actions.\n",
    "        bandit_env: An instance of the BanditEnvironment class set. Used for \n",
    "            taking actions, setting action-value vectors (i.e., contains k, \n",
    "            which we need for initializing q and n).\n",
    "        initial_estimates: The value used to populate the initial action-value\n",
    "        estimates vector (usually 0, can be higher for optimistic initial values).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, policy, bandit_env, initial_estimates = 0):\n",
    "        self.policy = policy\n",
    "        self.bandit_env = bandit_env\n",
    "        self.initial_estimates = initial_estimates # for easy reuse\n",
    "        self.action_value_estimates = self.initial_estimates * \\\n",
    "            np.ones(self.bandit_env.k)\n",
    "        self.action_counts = np.zeros(self.bandit_env.k)\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        It is generally good practice to use the __str__ method to enable\n",
    "        quick checking of relevant things about a class. Here, we might want to\n",
    "        check an Agent's policy type and number of actions.\n",
    "        \"\"\"\n",
    "        return \"Policy: {policy}, K = {k}\".format(\n",
    "            policy = self.policy.__str__(), \n",
    "            k = self.bandit_env.k\n",
    "        )\n",
    "    \n",
    "    def reset_action_value_estimates(self, new_initial_estimates = None):\n",
    "        \"\"\"\n",
    "        Resets the agent's action-value estimates and counts their initially\n",
    "        chose estimate (usualy 0), or to a new initial estimate passed as an\n",
    "        argument. All agents will need this, so we can put it here.\n",
    "        \n",
    "        Args:\n",
    "            new_initial_estimates: A value that will replace the attribute\n",
    "                initial_estimates in the reset.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def choose_action(self):\n",
    "        \"\"\"\n",
    "        Chooses an action to take against the environment. All agents will be\n",
    "        doing this, so we put it in the base/parent class.\n",
    "\n",
    "        Returns:\n",
    "            An action index.\n",
    "        \"\"\"\n",
    "        agent_data = {\n",
    "            \"action_value_estimates\" : self.action_value_estimates,\n",
    "            \"action_counts\" : self.action_counts\n",
    "        }\n",
    "        action = self.policy.choose_action(agent_data)\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "class SampleAverageBanditAgent(BanditAgent): # Note the inheritance\n",
    "    \"\"\"\n",
    "    A basic sample-average based bandit learner.\n",
    "\n",
    "    Note that because we included all the basic methods in the base class,\n",
    "    we do not need to repeat ourselves here. Much cleaner! All we need to do\n",
    "    is override the update_action_value_estimates function and we're done!\n",
    "    \"\"\"\n",
    "    def reset_action_value_estimates(self, new_initial_estimates = None):\n",
    "        if new_initial_estimates is not None:\n",
    "            self.action_value_estimates[:] = new_initial_estimates\n",
    "        else:\n",
    "            self.action_value_estimates[:] = self.initial_estimates\n",
    "        self.action_counts[:] = 0\n",
    "\n",
    "    def update_action_value_estimates(self, action, reward):\n",
    "        \"\"\"\n",
    "        Our basic sample-average incremental update, where the learning rate\n",
    "        is 1 / N(A)\n",
    "        \n",
    "        Args:\n",
    "            action: The index of the action to have its count and action-value\n",
    "            estimate updated.\n",
    "            reward: The reward received as a result of choosing the action above\n",
    "            that is used in the update equation. \n",
    "        \"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        learning_rate = 1 / self.action_counts[action]\n",
    "        self.action_value_estimates[action] += learning_rate * \\\n",
    "            (reward - self.action_value_estimates[action])\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "qKntK1ir5Tpk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class BanditTestRunner():\n",
    "    \"\"\"\n",
    "    This class object is the tool that runs our experiments and then visualizes\n",
    "    the results. We feed it ONE bandit environment, and one or more agents\n",
    "    that will attempt to solve that environment.\n",
    "\n",
    "    Note that in Figure 2.2 in the text, they use 1000 timesteps and average\n",
    "    over 2000 runs.\n",
    "\n",
    "    Attributes:\n",
    "        agents: One or more agents from the BanditAgent class set.\n",
    "        num_agents: The number of agents.\n",
    "        bandit_env: The BanditEnvironment object - the specific bandit problem\n",
    "            for this set of tests.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agents, bandit_env):\n",
    "        self.agents = agents\n",
    "        self.num_agents = len(agents)\n",
    "        self.bandit_env = bandit_env\n",
    "\n",
    "    def full_reset(self):\n",
    "        \"\"\"\n",
    "        Resets all true action values in the bandit environment, and resets\n",
    "        all action-value estimates and counts in the agents.\n",
    "        \"\"\"\n",
    "        self.bandit_env.reset_true_action_values()\n",
    "        for agent in self.agents:\n",
    "            agent.reset_action_value_estimates()\n",
    "        \n",
    "    def perform_runs(self, timesteps = 1000, runs = 2000, walk = None):\n",
    "        \"\"\"\n",
    "        Performs experiments in which the agent(s) attempt to learn the \n",
    "        bandit.\n",
    "\n",
    "        Args:\n",
    "            timesteps: The number of timesteps each agent has to learn the\n",
    "                        bandit within each run.\n",
    "            runs: The number of runs to conduct and average over.\n",
    "        \"\"\"\n",
    "        # One history per agent\n",
    "        rewards_history = np.zeros( (timesteps, len(self.agents)))\n",
    "        optimal_action_history = np.zeros_like(rewards_history)\n",
    "\n",
    "        for run in tqdm( range(0, runs) ):\n",
    "            self.full_reset()\n",
    "            \n",
    "            for timestep in range(0,timesteps):\n",
    "                \n",
    "                for agent_id, agent in enumerate(self.agents):\n",
    "                    \n",
    "                    # Choose your action\n",
    "                    action = agent.choose_action()\n",
    "                    \n",
    "                    # Act on the environment, receive reward signal.\n",
    "                    reward, optimal = self.bandit_env.\\\n",
    "                        take_action_emit_reward(action)\n",
    "                    \n",
    "                    # Use action and reward to update Q value.\n",
    "                    agent.update_action_value_estimates(action, reward)\n",
    "                    \n",
    "                    # Record reward and boolean for optimal choice, for later visualization.\n",
    "                    rewards_history[timestep, agent_id] += reward\n",
    "                    if optimal:\n",
    "                        optimal_action_history[timestep,agent_id] += 1\n",
    "                    if walk is not None:\n",
    "                        self.bandit_env.random_walk_action_values(walk[0],walk[1])\n",
    "                \n",
    "                \n",
    "        return rewards_history / runs, optimal_action_history / runs\n",
    "\n",
    "    def visualize_results(self, save_filename, title, rewards_histories,\n",
    "        optimal_action_histories): \n",
    "        # To label our plots, we reference our list of agents.\n",
    "        # Our history objects will also be in this order.\n",
    "        # So we can call the policy __str__ function to label our plot.\n",
    "        labels = []\n",
    "        for agent in self.agents:\n",
    "            labels.append(agent.policy.__str__())\n",
    "\n",
    "        plt.figure(figsize=[16,12])\n",
    "        \n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(optimal_action_histories*100)\n",
    "        print(optimal_action_histories)\n",
    "        print(optimal_action_histories*100)\n",
    "        plt.ylim(0,100)\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"% Optimal Action Chosen\")\n",
    "        plt.xticks( np.arange(0, len(rewards_histories), len(rewards_histories)/10) )\n",
    "        plt.tick_params(axis='x')\n",
    "        plt.tick_params(axis='y')\n",
    "        plt.legend(labels)\n",
    "        \n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(rewards_histories)\n",
    "        plt.ylabel(\"Average Reward\")\n",
    "        plt.xticks( np.arange(0, len(rewards_histories),len(rewards_histories)/10) )\n",
    "        plt.tick_params(axis='x')\n",
    "        plt.tick_params(axis='y')\n",
    "        plt.legend(labels)\n",
    "        plt.show\n",
    "\n",
    "        if save_filename is not None:\n",
    "            plt.savefig(save_filename)    "
   ],
   "outputs": [],
   "metadata": {
    "id": "BRBsq2FD5Ye6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "bandit = BanditEnvironment(k = 10, mu = 0, sigma = 1)\n",
    "\n",
    "egreedy_10perc = BanditEpsilonGreedyPolicy()\n",
    "egreedy_1perc = BanditEpsilonGreedyPolicy(epsilon = 0.01)\n",
    "### initialize a UCB policy\n",
    "ucb_1perc = BanditUCBPolicy(c = 1)\n",
    "ucb_1_5perc = BanditUCBPolicy(c = 1.5)\n",
    "ucb_2perc = BanditUCBPolicy(c = 2)\n",
    "ucb_2_5perc = BanditUCBPolicy(c = 2.5)\n",
    "ucb_3perc = BanditUCBPolicy(c = 3)\n",
    "ucb_3_5perc = BanditUCBPolicy(c = 3.5)\n",
    "ucb_4perc = BanditUCBPolicy(c = 4)\n",
    "ucb_5perc = BanditUCBPolicy(c = 5)\n",
    "\n",
    "\n",
    "agent1 = SampleAverageBanditAgent(policy = egreedy_10perc, bandit_env = bandit)\n",
    "agent2 = SampleAverageBanditAgent(policy = egreedy_1perc, bandit_env = bandit)\n",
    "### Initialize SampleAverageBanditAgent and give it the UCB policy\n",
    "agent3 = SampleAverageBanditAgent(policy = ucb_1perc, bandit_env = bandit)\n",
    "agent4 = SampleAverageBanditAgent(policy = ucb_1_5perc, bandit_env = bandit)\n",
    "agent5 = SampleAverageBanditAgent(policy = ucb_2perc, bandit_env = bandit)\n",
    "agent6 = SampleAverageBanditAgent(policy = ucb_2_5perc, bandit_env = bandit)\n",
    "agent7 = SampleAverageBanditAgent(policy = ucb_3perc, bandit_env = bandit)\n",
    "\n",
    "agents = [agent1, agent3, agent4, agent5, agent6, agent7]\n",
    "\n",
    "runner = BanditTestRunner(agents = agents, bandit_env = bandit)\n",
    "\n",
    "# Note how long it takes to perform the runs - very good case for parallelization!\n",
    "r, o = runner.perform_runs(timesteps = 1000, runs = 2000)\n",
    "\n",
    "### edit title\n",
    "runner.visualize_results(\n",
    "    save_filename = None,\n",
    "        title = \"Epsilon-Greedy 10% vs. Epsilon-Greedy 1%\", \n",
    "        rewards_histories = r, \n",
    "        optimal_action_histories = o\n",
    "        )\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 18%|█▊        | 369/2000 [21:01:27<2995:19:56, 6611.40s/it]"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "WdsU92YZ5a0b",
    "outputId": "5b6c42b7-7d59-4ba7-a27d-251cf93a3e8f",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "r, o = runner.perform_runs(timesteps = 10000, runs = 1000, walk = [0,0.01])\n",
    "\n",
    "runner.visualize_results(\n",
    "    save_filename = None,\n",
    "        title = \"Epsilon-Greedy 10% vs. UCB C=2 for Random Walk N(0,0.01)\", \n",
    "        rewards_histories = r, \n",
    "        optimal_action_histories = o\n",
    "        )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 11%|â–ˆ         | 112/1000 [17:33<3:02:28, 12.33s/it]"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "S0Ek3bJk5foT",
    "outputId": "8b1fe911-cd2a-4d73-efe9-aa3a0b52b11c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "Rg7Fd5wA-EJy"
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS9670B - Assignment 1 Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}