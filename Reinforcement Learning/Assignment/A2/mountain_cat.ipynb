{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import gym\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from gym import error, spaces, utils\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "print(env.action_space.n)\n",
    "\n",
    "# there are only two observables - position and velocity\n",
    "print(env.observation_space.high)  # the high values of the observations\n",
    "print(env.observation_space.low)  # the low values"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "env.action_space.n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "num_states = (env.observation_space.high - env.observation_space.low) * np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "num_states"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([19, 15])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "Q.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19, 15, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "'''\n",
    "        The MountainCar env:\n",
    "\n",
    "        Observation Space: there are two observables - position and velocity\n",
    "        position of the car along the x-axis  in range [-1.2, 0.6]\n",
    "        velocity of the car  in range [-0.07, 0.07]\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n        The MountainCar env:\\n\\n        Observation Space: there are two observables - position and velocity\\n        position of the car along the x-axis  in range [-1.2, 0.6]\\n        velocity of the car  in range [-0.07, 0.07]\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "class Q_learning_agent(object):\n",
    "    def __init__(self, obs, act_n = 3, learning_rate=0.01, gamma=0.9, e_greed=0.1):\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = e_greed\n",
    "\n",
    "        '''\n",
    "        There are 19 * 15 observation space in total\n",
    "        19 means there are 19 different x-posoition\n",
    "        15 means there are 15 different velocity\n",
    "        num_state = [0.6 - (-1.2), 0.07 - (-0.07)] * [10, 100] + 1 = [18, 14] + 1 = [19, 15]\n",
    "        '''\n",
    "        n_states = (obs.high - obs.low) * np.array([10, 100])\n",
    "        self.num_states = np.round(n_states, 0).astype(int) + 1\n",
    "\n",
    "        '''\n",
    "        There are 3 action in action space\n",
    "        0: Accelerate to the left\n",
    "        1: Don't accelerate  \n",
    "        2: Accelerate to the right\n",
    "        '''\n",
    "        self.act_n = act_n\n",
    "\n",
    "        '''\n",
    "        Q as a shape of (19, 15, 3)\n",
    "        '''\n",
    "        self.Q = np.random.uniform(low = -1, high = 1, size = (self.num_states[0], self.num_states[1], self.act_n))\n",
    "        \n",
    "    \n",
    "    def policy(self, state):\n",
    "        if np.random.uniform(0, 1) < (1.0 - self.epsilon):\n",
    "            action = self.predict(state)\n",
    "        else:\n",
    "            action = np.random.choice(self.act_n)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def predict(self, state):\n",
    "        action = np.argmax(Q[state[0], state[1]]) \n",
    "        return action\n",
    "        \n",
    "    def learn(self, state, action, reward, state_prime, action_prime, done):\n",
    "        predit_Q = self.Q[state[0], state[1], action]\n",
    "\n",
    "        if (done):\n",
    "            target_Q = reward\n",
    "        else:\n",
    "            target_Q = reward + self.gamma * np.max(self.Q[state_prime[0], state_prime[1]])\n",
    "\n",
    "        self.Q[state[0], state[1], action] += self.lr * (target_Q - predit_Q)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def run_episode(env, agent, is_render):\n",
    "    state = env.reset()\n",
    "    state = (state - env.observation_space.low)*np.array([10, 100])\n",
    "    state = np.round(state, 0).astype(int)\n",
    "\n",
    "    action = agent.policy(state)\n",
    "    \n",
    "    total_reward = 0\n",
    "    while (True):\n",
    "        if (is_render):\n",
    "            env.render()\n",
    "            \n",
    "        S_prime, reward, done, _ = env.step(action)\n",
    "\n",
    "        S_prime = (S_prime - env.observation_space.low)*np.array([10, 100])\n",
    "        S_prime = np.round(S_prime, 0).astype(int)\n",
    "\n",
    "        action_prime = agent.policy(S_prime)\n",
    "\n",
    "        agent.learn(state, action, reward, S_prime, action_prime, done)\n",
    "\n",
    "        action = action_prime\n",
    "        state = S_prime\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def main():\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "    q_agent = Q_learning_agent(obs=env.observation_space,  act_n=env.action_space.n, \n",
    "        learning_rate=0.5, gamma = 0.9, e_greed=0.1)\n",
    "\n",
    "    q_reward_list = []\n",
    "\n",
    "    is_render = False\n",
    "\n",
    "    for episode in range(500):\n",
    "        if (episode%20 == 0):\n",
    "            is_render = True\n",
    "        q_reward = run_episode(env, q_agent, is_render)\n",
    "        q_reward_list.append(q_reward)\n",
    "\n",
    "    return q_reward_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "reward_history = main()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/97/x83b8hr53hlb306v1n23j00c0000gn/T/ipykernel_61806/3040250582.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreward_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/97/x83b8hr53hlb306v1n23j00c0000gn/T/ipykernel_61806/1101046648.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MountainCar-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     q_agent = Q_learning_agent(obs=env.observation_space,  act_n=env.action_space.n, \n\u001b[0m\u001b[1;32m      5\u001b[0m         learning_rate=0.5, gamma = 0.9, e_greed=0.1)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/97/x83b8hr53hlb306v1n23j00c0000gn/T/ipykernel_61806/4230631698.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obs, act_n, learning_rate, gamma, e_greed)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         '''\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.uniform\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}