{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Perform one STEP of the value iteration algorithm for the underlined state (second row, second column). In other words, calculate the state value estimate V(s) for\n",
    "V(2,2) using the formula in the value iteration algorithm as in the text. Show your work with written arithmetic. \n",
    "\n",
    "$$\n",
    "V(2, 2) = 0.7 * [0.5 * (0 + 0.9 * 0.3) +\n",
    " 0.25 * (0 + 0.9 * -0.1) +\n",
    " 0.25 * (-1 + 0.9 * -1)] +\n",
    "\n",
    " 0.1 * [0.5 * (0 + 0.9 * -0.1) +\n",
    " 0.25 * (0 + 0.9 * 0.3) +\n",
    " 0.25 * (0 + 0.9 * -0.3)] +\n",
    "\n",
    " 0.1 * [0.5 * (0 + 0.9 * -0.3) +\n",
    " 0.25 * (0 + 0.9 * -0.1) +\n",
    " 0.25 * (-1 + 0.9 * -1)] +\n",
    "\n",
    " 0.1 * [0.5 * (-1 + 0.9 * -1) +\n",
    " 0.25 * (0 + 0.9 * -0.3) +\n",
    " 0.25 * (0 + 0.9 * -0.3)]\n",
    " = -0.43\n",
    "$$\n",
    "\n",
    "Working again from the base value estimates above: your agent is in state [2,2] and oves right, not slipping. Complete the Expected SARSA update for Q([2,2],”right”). Show your work with written arithmetic.\n",
    "\n",
    "$\n",
    "Q(s,a) = Q(s,a) + \\alpha [R + \\gamma \\sum \\pi(a|s^\\prime)Q(s^\\prime, a) - Q(s,a)]\\\\\n",
    "&= Q(2,2) + 0.2 [0 + 0.9 * (0.7* Q(2,3) - Q(2, 2)) \n",
    "\n",
    "+ 0.9 * (0.1 * Q(2,1) - Q(2,2)) + 0.9 * (0.1 * Q(1, 2) - Q(2, 2)) + 0.9 * (0.1 * Q(3, 1) - Q(2, 2))\n",
    "]\n",
    "$\n",
    "\n",
    "Working again from the base value estimates above: your agent is in state [2,2] and moves right, not slipping. Complete the Expected SARSA update for Q([2,2],”right”). Show your work with written arithmetic.\n",
    "\n",
    "Q(2,2) = \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "0.7 * (0.5 * (0 + 0.9 * 0.3) +0.25 * (0 + 0.9 * -0.1) + 0.25 * (-1 + 0.9 * -1)) + \\\n",
    "        0.1 * (0.5 * (0 + 0.9 * -0.1) + 0.25 * (0 + 0.9 * 0.3) + 0.25 * (0 + 0.9 * -0.3)) +\\\n",
    "            0.1 * (0.5 * (0 + 0.9 * -0.3) + 0.25 * (0 + 0.9 * -0.1) + 0.25 * (-1 + 0.9 * -1)) +\\\n",
    "                 0.1 * (0.5 * (-1 + 0.9 * -1) + 0.25 * (0 + 0.9 * -0.3) + 0.25 * (0 + 0.9 * -0.3))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.43"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In your own words, describe on-policy learning, off-policy learning, the difference between them, and one practical real-world scenario where you would prefer off-policy learning (2 marks; moderate detail).\n",
    "\n",
    "On policy methods attempt to evaluate or improve the policy that is used to make decisions, \n",
    "\n",
    "Off-policy methods evaluate or improve a policy different\n",
    "from that used to generate the data.\n",
    "\n",
    "Off-policy has target policy and behavior policy, where the target policy learning its value function is the target of the learning process, behavior policy is the policy controlling the agent and generating behavior. In on policy, exploration must be part of the single policy. However, In off-policy, exploration can be isolated just to the learning policy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Briefly describe one practical, real-world scenario where you would want to use a uniform random policy (0.5 marks; no need to go into detail).\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In your own words, provide the reinforcement learning definitions of prediction and control\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is the difference between SARSA and Q-Learning? (b) Provide one practical, real-world scenario where this difference could matter\n",
    "\n",
    "Q-Learning is an off-policy algorithm that uses only the maximal available action in its updates regardless of the action that was actually taken.\n",
    "\n",
    "Because Q-Learning focuses on the optimal action only, it will be willing to take risks and learn potentially more dangerous paths to success.\n",
    "\n",
    "For example, when the  risky behaviour is not a problem, like in a war computer game, the q-learning agent will be a better choise.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The year is 2033 and you work for an agribusiness MegaCorp. You are tasked with defining the environment to run a farm with no human intervention"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, you look to define the state space. What might you include? Can you ensure the environment is Markov?\n",
    "\n",
    "The state may include:\n",
    "- Number of plants seed\n",
    "- Current load of the storage room\n",
    "- Number of machine\n",
    "- Weater of the day\n",
    "\n",
    "\n",
    "Next, you define the action space. What decisions might need to be made on a farm? Describe how you might use RL to competently make the decisions around a farm in a day. (don’t worry about knowing anything about farms here\n",
    "- let’s say there are no animals, just plants, and there is no company prescription about what to grow, where, when, how to maintain the plants such that yield is high, maintain the farm, etc.)\n",
    "- plant the seed\n",
    "- watering\n",
    "- harvest\n",
    "- maintenance machine\n",
    "- fertilize\n",
    "- sell product\n",
    "\n",
    "Finally, you consider the reward function. What would your agent(s) need to find rewarding?\n",
    "\n",
    "+1 for each product is produced\n",
    "+1 for each product is selled\n",
    "-1 for each plant die\n",
    "-5 for each machine not work\n",
    "-10 for the stroage room is pull and no room for newly producted plant.\n",
    "\n",
    "Would you want to use supervised learning for any part of your autonomous farm project? Why or why not?\n",
    "\n",
    "I would not use supervised learning for autonomous farm project. Since RL cares about the  $P(S_{t+1}|S_{t})$, which means each decision in rl has consequences. However, supervised learning only cares about the probability of the target given the inputs, where the next data is has nothing at all to do with the one that came before. Therefore supervised learning did well on image data. In this scenario, each decision make in the farm has consequences, so I will not use supervised learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "0.27 + 0.2 * (0.9 * 0.28 - 0.27)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.2664"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}