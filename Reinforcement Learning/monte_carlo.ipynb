{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import gym\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from gym import error, spaces, utils\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "class CliffWalkingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A simplified implementation of the cliffwalking environment from \n",
    "    Sutton & Barto (e.g., Figure 6.13)\n",
    "\n",
    "    The board is a 4x12 matrix, with (using NumPy matrix indexing):\n",
    "        [3, 0] as the start at bottom-left\n",
    "        [3, 11] as the goal at bottom-right\n",
    "        [3, 1..10] as the cliff at bottom-center\n",
    "\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "    \"\"\"\n",
    "\n",
    "    def observation(self, state):\n",
    "        return state[0] * self.cols + state[1]\n",
    "\n",
    "    def __init__(self, render_steps = False):\n",
    "        self.env_shape = (4, 12) # attributes like these are added for convenience\n",
    "        # (i.e., so it's easy to refer to these values in various methods)\n",
    "        self.rows = 4\n",
    "        self.cols = 12\n",
    "        self.start = [3,0] # row, col of our start position\n",
    "        self.goal = [3,11] # row, col of our terminal state\n",
    "        self.current_state = self.start \n",
    "        self.render_steps = render_steps\n",
    "\n",
    "        # Cliff location\n",
    "        self._cliff = np.zeros(self.env_shape, dtype=np.bool)\n",
    "        self._cliff[3, 1:-1] = True \n",
    "\n",
    "        # There are four actions: up, down, left and right\n",
    "        # Note that the state space gym.spaces.Discrete is a custom class\n",
    "        # in the gym library.\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # observation is the x, y coordinate of the grid\n",
    "        # Note the state space is the same custom class.\n",
    "        self.observation_space = spaces.Discrete(self.rows*self.cols)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the cliffwalking environment.\n",
    "\n",
    "        INPUT\n",
    "          action : integer (0,3) : right (0), down (1), left (2), and up (3).\n",
    "        OUTPUT\n",
    "          observation : A special class representing the x,y on the grid.\n",
    "          reward : float : The reward received following the action.\n",
    "          terminal_state : boolean : Has the terminal state been reached?\n",
    "        \"\"\"\n",
    "        new_state = deepcopy(self.current_state)\n",
    "\n",
    "        if action == 0: #right, \"min\" ensures you can't go through a wall,\n",
    "          # and just end up back in your previous state.\n",
    "            new_state[1] = min(new_state[1]+1, self.cols-1)\n",
    "        elif action == 1: #down, \"max\" ensures you can't go through a wall.\n",
    "            new_state[0] = max(new_state[0]-1, 0)\n",
    "        elif action == 2: #left\n",
    "            new_state[1] = max(new_state[1]-1, 0)\n",
    "        elif action == 3: #up\n",
    "            new_state[0] = min(new_state[0]+1, self.rows-1)\n",
    "        else:\n",
    "            raise Exception(\"Invalid action.\")\n",
    "\n",
    "        # Update our state\n",
    "        self.current_state = new_state\n",
    "\n",
    "        if self.render_steps:\n",
    "          self.render()\n",
    "\n",
    "        reward = -1.0\n",
    "        terminal_state = False\n",
    "        if self.current_state[0] == 3 and self.current_state[1] > 0:\n",
    "            if self.current_state[1] < self.cols - 1:\n",
    "                reward = -100.0\n",
    "                self.current_state = deepcopy(self.start)\n",
    "            else:\n",
    "                terminal_state = True\n",
    "\n",
    "        # IMPORTANT: The step() method needs to return these 4 things in this order.\n",
    "        # The new state, the reward, \"is this episode over?\" boolean\", \n",
    "        # and an \"info dictionary\" for debugging purposes if you want to use it. \n",
    "        # I leave it empty.  \n",
    "        return self.observation(self.current_state), reward, terminal_state, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.start\n",
    "        return self.observation(self.current_state)\n",
    "    \n",
    "    def show_env(self):\n",
    "      \"\"\"\n",
    "      Shows the environment.\n",
    "      \"\"\"\n",
    "      gridworld = np.ndarray((self.rows,self.cols), dtype = 'object')\n",
    "\n",
    "      for row in np.arange(self.rows):\n",
    "        for col in np.arange(self.cols):\n",
    "\n",
    "          if self.current_state == [row,col]:\n",
    "            gridworld[row,col] = \"x\"\n",
    "          elif [row,col] == self.goal:\n",
    "              gridworld[row,col] = \"T\"\n",
    "          elif self._cliff[row,col]:\n",
    "              gridworld[row,col] = \"C\"\n",
    "          else:\n",
    "              gridworld[row,col] = \"o\"\n",
    "\n",
    "      print(gridworld)\n",
    "\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        self._render(mode, close)\n",
    "\n",
    "    def _render(self, mode='human', close=False):\n",
    "      \"\"\"\n",
    "      render function from:\n",
    "      https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py\n",
    "\n",
    "      I admit I don't have much experience with rendering with characters. On face,\n",
    "      this works the same as what I made.\n",
    "      \"\"\"\n",
    "      if close:\n",
    "        return\n",
    "\n",
    "      outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "      for s in range(self.rows*self.cols):\n",
    "            position = np.unravel_index(s, self.env_shape)\n",
    "            # print(self.s)\n",
    "            if self.current_state == s:\n",
    "                output = \" x \"\n",
    "            elif position == (3,11):\n",
    "                output = \" T \"\n",
    "            elif self._cliff[position]:\n",
    "                output = \" C \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip() \n",
    "            if position[1] == self.env_shape[1] - 1:\n",
    "                output = output.rstrip() \n",
    "                output += \"\\n\"\n",
    "\n",
    "            outfile.write(output)\n",
    "      outfile.write(\"\\n\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "class monte_carlo_agent(object):\n",
    "    '''\n",
    "    A first visit Monte Carlo agent with epsilon greedy or epsilon soft policy\n",
    "    '''\n",
    "    def __init__(self, obs_n, act_n, epsilon, gamma):\n",
    "        self.epsilon = epsilon\n",
    "        self.act_n = act_n\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.total_return = defaultdict(lambda:0)\n",
    "        self.num_visits = defaultdict(lambda:0)\n",
    "\n",
    "        self.Q = np.zeros((obs_n, act_n))\n",
    "\n",
    "    def optimal_action(self, S):\n",
    "        Q_values = self.Q[S]\n",
    "        maxQ = np.max(self.Q[S])\n",
    "\n",
    "        action = np.random.choice(np.where(Q_values == maxQ)[0])\n",
    "        return action\n",
    "\n",
    "    def epsilon_greedy_policy(self, S):\n",
    "        '''\n",
    "        A epsilon greedy policy\n",
    "        a = argmax_a Q with probability 1 - epsilon\n",
    "        a = random action with probability epsilon\n",
    "        '''\n",
    "        if np.random.uniform(0, 1) < (1.0 - self.epsilon):\n",
    "            action = self.optimal_action(S)\n",
    "        else:\n",
    "            action = np.random.choice(self.act_n)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def epsilon_soft_policy(self, S, action=None):\n",
    "        \"\"\"\n",
    "        A epsilon soft policy\n",
    "        pi(a|s) = 1 - epsilon + (epsilon / |A(s)|) if a == a*\n",
    "        pi(a|s) = epsilon / |A(s)|                 if a != a*\n",
    "        \"\"\"\n",
    "\n",
    "        pi = np.ones(self.act_n) * (self.epsilon / self.act_n)\n",
    "        pi[self.optimal_action(S)] = self.epsilon / self.act_n\n",
    "\n",
    "        choose_action = np.random.multinomial(1, pi)\n",
    "        action = np.random.choice(np.where(choose_action == 1)[0])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, states, actions, rewards):\n",
    "        first_visit_list = []\n",
    "        for step, state in enumerate(states):\n",
    "            if (state not in first_visit_list):\n",
    "                first_visit_list.append(state)\n",
    "                \n",
    "                visited_rewards = rewards[step:]\n",
    "                discounts = self.gamma ** np.arange(0, len(visited_rewards))\n",
    "\n",
    "                action = actions[step]\n",
    "                self.total_return[state, action] += sum(discounts * visited_rewards)\n",
    "                self.num_visits[state, action] += 1\n",
    "                self.Q[state, action] = self.total_return[state, action] / self.num_visits[state, action]\n",
    "\n",
    "       "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "'''\n",
    "run_episode run an episode and return all states, actions and rewards\n",
    "this episode visited.\n",
    "'''\n",
    "def run_episode(env, agent, policy):\n",
    "    trajectory = []\n",
    "    S = env.reset()\n",
    "\n",
    "    while(True):\n",
    "        if (policy == \"epsilon_greedy\"):\n",
    "            action = agent.epsilon_greedy_policy(S)\n",
    "        elif (policy == \"epsilon_soft\"):\n",
    "            action = agent.epsilon_soft_policy(S)\n",
    "\n",
    "        S_prime, reward, done, _ = env.step(action)\n",
    "        trajectory.append((S, action, reward))\n",
    "\n",
    "        S = S_prime\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return zip(*trajectory)\n",
    "\n",
    "def monte_carlo(policy):\n",
    "    env = CliffWalkingEnv()\n",
    "\n",
    "    mc_agent = monte_carlo_agent(obs_n=env.observation_space.n,  act_n=env.action_space.n,  gamma = 0.9, epsilon=0.2)\n",
    "\n",
    "    rewards_list = np.zeros(500)\n",
    "\n",
    "    for episode in tqdm(range(500)):\n",
    "        states, actions, rewards = run_episode(env, mc_agent, policy)\n",
    "        mc_agent.learn(states, actions, rewards)\n",
    "        rewards_list[episode] = sum(rewards)\n",
    "\n",
    "    return rewards_list\n",
    "\n",
    "rl = monte_carlo(\"epsilon_greedy\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 500/500 [06:30<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "plt.plot(range(0, 500), rl)\n",
    "plt.ylim(-1000, 0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(-200.0, 0.0)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}