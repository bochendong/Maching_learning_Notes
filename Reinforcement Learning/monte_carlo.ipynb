{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "import gym\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from gym import error, spaces, utils\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "class CliffWalkingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A simplified implementation of the cliffwalking environment from \n",
    "    Sutton & Barto (e.g., Figure 6.13)\n",
    "\n",
    "    The board is a 4x12 matrix, with (using NumPy matrix indexing):\n",
    "        [3, 0] as the start at bottom-left\n",
    "        [3, 11] as the goal at bottom-right\n",
    "        [3, 1..10] as the cliff at bottom-center\n",
    "\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "    \"\"\"\n",
    "\n",
    "    def observation(self, state):\n",
    "        return state[0] * self.cols + state[1]\n",
    "\n",
    "    def __init__(self, render_steps = False):\n",
    "        self.env_shape = (4, 12) # attributes like these are added for convenience\n",
    "        # (i.e., so it's easy to refer to these values in various methods)\n",
    "        self.rows = 4\n",
    "        self.cols = 12\n",
    "        self.start = [3,0] # row, col of our start position\n",
    "        self.goal = [3,11] # row, col of our terminal state\n",
    "        self.current_state = self.start \n",
    "        self.render_steps = render_steps\n",
    "\n",
    "        # Cliff location\n",
    "        self._cliff = np.zeros(self.env_shape, dtype=np.bool)\n",
    "        self._cliff[3, 1:-1] = True \n",
    "\n",
    "        # There are four actions: up, down, left and right\n",
    "        # Note that the state space gym.spaces.Discrete is a custom class\n",
    "        # in the gym library.\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # observation is the x, y coordinate of the grid\n",
    "        # Note the state space is the same custom class.\n",
    "        self.observation_space = spaces.Discrete(self.rows*self.cols)\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the cliffwalking environment.\n",
    "\n",
    "        INPUT\n",
    "          action : integer (0,3) : right (0), down (1), left (2), and up (3).\n",
    "        OUTPUT\n",
    "          observation : A special class representing the x,y on the grid.\n",
    "          reward : float : The reward received following the action.\n",
    "          terminal_state : boolean : Has the terminal state been reached?\n",
    "        \"\"\"\n",
    "        new_state = deepcopy(self.current_state)\n",
    "\n",
    "        if action == 0: #right, \"min\" ensures you can't go through a wall,\n",
    "          # and just end up back in your previous state.\n",
    "            new_state[1] = min(new_state[1]+1, self.cols-1)\n",
    "        elif action == 1: #down, \"max\" ensures you can't go through a wall.\n",
    "            new_state[0] = max(new_state[0]-1, 0)\n",
    "        elif action == 2: #left\n",
    "            new_state[1] = max(new_state[1]-1, 0)\n",
    "        elif action == 3: #up\n",
    "            new_state[0] = min(new_state[0]+1, self.rows-1)\n",
    "        else:\n",
    "            raise Exception(\"Invalid action.\")\n",
    "\n",
    "        # Update our state\n",
    "        self.current_state = new_state\n",
    "\n",
    "        if self.render_steps:\n",
    "          self.render()\n",
    "\n",
    "        reward = -1.0\n",
    "        terminal_state = False\n",
    "        if self.current_state[0] == 3 and self.current_state[1] > 0:\n",
    "            if self.current_state[1] < self.cols - 1:\n",
    "                reward = -100.0\n",
    "                self.current_state = deepcopy(self.start)\n",
    "            else:\n",
    "                terminal_state = True\n",
    "\n",
    "        # IMPORTANT: The step() method needs to return these 4 things in this order.\n",
    "        # The new state, the reward, \"is this episode over?\" boolean\", \n",
    "        # and an \"info dictionary\" for debugging purposes if you want to use it. \n",
    "        # I leave it empty.  \n",
    "        return self.observation(self.current_state), reward, terminal_state, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.start\n",
    "        return self.observation(self.current_state)\n",
    "    \n",
    "    def show_env(self):\n",
    "      \"\"\"\n",
    "      Shows the environment.\n",
    "      \"\"\"\n",
    "      gridworld = np.ndarray((self.rows,self.cols), dtype = 'object')\n",
    "\n",
    "      for row in np.arange(self.rows):\n",
    "        for col in np.arange(self.cols):\n",
    "\n",
    "          if self.current_state == [row,col]:\n",
    "            gridworld[row,col] = \"x\"\n",
    "          elif [row,col] == self.goal:\n",
    "              gridworld[row,col] = \"T\"\n",
    "          elif self._cliff[row,col]:\n",
    "              gridworld[row,col] = \"C\"\n",
    "          else:\n",
    "              gridworld[row,col] = \"o\"\n",
    "\n",
    "      print(gridworld)\n",
    "\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        self._render(mode, close)\n",
    "\n",
    "    def _render(self, mode='human', close=False):\n",
    "      \"\"\"\n",
    "      render function from:\n",
    "      https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py\n",
    "\n",
    "      I admit I don't have much experience with rendering with characters. On face,\n",
    "      this works the same as what I made.\n",
    "      \"\"\"\n",
    "      if close:\n",
    "        return\n",
    "\n",
    "      outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "      for s in range(self.rows*self.cols):\n",
    "            position = np.unravel_index(s, self.env_shape)\n",
    "            # print(self.s)\n",
    "            if self.current_state == s:\n",
    "                output = \" x \"\n",
    "            elif position == (3,11):\n",
    "                output = \" T \"\n",
    "            elif self._cliff[position]:\n",
    "                output = \" C \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip() \n",
    "            if position[1] == self.env_shape[1] - 1:\n",
    "                output = output.rstrip() \n",
    "                output += \"\\n\"\n",
    "\n",
    "            outfile.write(output)\n",
    "      outfile.write(\"\\n\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "class monte_carlo_agent(object):\n",
    "    '''\n",
    "    A first visit Monte Carlo agent with epsilon greedy or epsilon soft policy\n",
    "    '''\n",
    "    def __init__(self, obs_n, act_n, epsilon, gamma):\n",
    "        self.epsilon = epsilon\n",
    "        self.act_n = act_n\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.total_return = defaultdict(lambda:0)\n",
    "        self.num_visits = defaultdict(lambda:0)\n",
    "\n",
    "        self.Q = np.zeros((obs_n, act_n))\n",
    "\n",
    "    def optimal_action(self, S):\n",
    "        Q_values = self.Q[S]\n",
    "        maxQ = np.max(self.Q[S])\n",
    "\n",
    "        action = np.random.choice(np.where(Q_values == maxQ)[0])\n",
    "        return action\n",
    "\n",
    "    def epsilon_greedy_policy(self, S):\n",
    "        '''\n",
    "        A epsilon greedy policy\n",
    "        a = argmax_a Q with probability 1 - epsilon\n",
    "        a = random action with probability epsilon\n",
    "        '''\n",
    "        if np.random.uniform(0, 1) < (1.0 - self.epsilon):\n",
    "            action = self.optimal_action(S)\n",
    "        else:\n",
    "            action = np.random.choice(self.act_n)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def epsilon_soft_policy(self, S, action=None):\n",
    "        \"\"\"\n",
    "        A epsilon soft policy\n",
    "        pi(a|s) = 1 - epsilon + (epsilon / |A(s)|) if a == a*\n",
    "        pi(a|s) = epsilon / |A(s)|                 if a != a*\n",
    "        \"\"\"\n",
    "\n",
    "        pi = np.ones(self.act_n) * (self.epsilon / self.act_n)\n",
    "        pi[self.optimal_action(S)] = self.epsilon / self.act_n\n",
    "\n",
    "        choose_action = np.random.multinomial(1, pi)\n",
    "        action = np.random.choice(np.where(choose_action == 1)[0])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, states, actions, rewards):\n",
    "        for step, state in enumerate(states):\n",
    "            visited_rewards = rewards[step:]\n",
    "            discounts = self.gamma ** np.arange(0, len(visited_rewards))\n",
    "\n",
    "            action = actions[step]\n",
    "            self.total_return[state, action] += sum(discounts * visited_rewards)\n",
    "            self.num_visits[state, action] += 1\n",
    "            self.Q[state, action] = self.total_return[state, action] / self.num_visits[state, action]\n",
    "       "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "'''\n",
    "run_episode run an episode and return all states, actions and rewards\n",
    "this episode visited.\n",
    "'''\n",
    "def run_episode(env, agent, policy):\n",
    "    trajectory = []\n",
    "    S = env.reset()\n",
    "\n",
    "    while(True):\n",
    "        if (policy == \"epsilon_greedy\"):\n",
    "            action = agent.epsilon_greedy_policy(S)\n",
    "        elif (policy == \"epsilon_soft\"):\n",
    "            action = agent.epsilon_soft_policy(S)\n",
    "\n",
    "        S_prime, reward, done, _ = env.step(action)\n",
    "        trajectory.append((S, action, reward))\n",
    "\n",
    "        S = S_prime\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return zip(*trajectory)\n",
    "\n",
    "def monte_carlo(policy):\n",
    "    env = CliffWalkingEnv()\n",
    "\n",
    "    mc_agent = monte_carlo_agent(obs_n=env.observation_space.n,  act_n=env.action_space.n,  gamma = 0.8, epsilon=0.2)\n",
    "\n",
    "    rewards_list = np.zeros(500)\n",
    "\n",
    "    for episode in tqdm(range(500)):\n",
    "        print(episode)\n",
    "        states, actions, rewards = run_episode(env, mc_agent, policy)\n",
    "        mc_agent.learn(states, actions, rewards)\n",
    "        rewards_list[episode] = sum(rewards)\n",
    "\n",
    "    return rewards_list\n",
    "\n",
    "rl = monte_carlo(\"epsilon_greedy\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1/500 [00:01<12:16,  1.48s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/500 [01:18<6:21:30, 45.96s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/500 [04:03<16:49:27, 121.62s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/97/x83b8hr53hlb306v1n23j00c0000gn/T/ipykernel_51357/841328240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrewards_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonte_carlo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epsilon_greedy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/97/x83b8hr53hlb306v1n23j00c0000gn/T/ipykernel_51357/841328240.py\u001b[0m in \u001b[0;36mmonte_carlo\u001b[0;34m(policy)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mmc_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mrewards_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/97/x83b8hr53hlb306v1n23j00c0000gn/T/ipykernel_51357/841328240.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, agent, policy)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_soft_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mS_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/97/x83b8hr53hlb306v1n23j00c0000gn/T/ipykernel_51357/821164196.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# and an \"info dictionary\" for debugging purposes if you want to use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# I leave it empty.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(range(0, 500), rl)\n",
    "plt.ylim(-200,0)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'esmc_rl' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/97/x83b8hr53hlb306v1n23j00c0000gn/T/ipykernel_51357/1123847875.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mesmc_rl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'esmc_rl' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}